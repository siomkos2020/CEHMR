{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read from MIMIC csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sunmengxuan/anaconda3/envs/smx/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3343: DtypeWarning: Columns (11) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#patients  (6350,)\n",
      "#clinical events  15016\n",
      "#diagnosis  1958\n",
      "#med  145\n",
      "#procedure 1426\n",
      "#avg of diagnoses  10.514717634523175\n",
      "#avg of medicines  8.80420884389984\n",
      "#avg of procedures  3.8445657964837507\n",
      "#avg of vists  2.3647244094488187\n",
      "#max of diagnoses  128\n",
      "#max of medicines  55\n",
      "#max of procedures  50\n",
      "#max of visit  29\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SUBJECT_ID</th>\n",
       "      <th>HADM_ID</th>\n",
       "      <th>ICD9_CODE</th>\n",
       "      <th>NDC</th>\n",
       "      <th>PRO_CODE</th>\n",
       "      <th>NDC_Len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17</td>\n",
       "      <td>161087</td>\n",
       "      <td>[4239, 5119, 78551, 4589, 311, 7220, 71946, 2724]</td>\n",
       "      <td>[N02B, A01A, A02B, A06A, B05C, A12A, A12C, C01...</td>\n",
       "      <td>[3731, 8872, 3893]</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17</td>\n",
       "      <td>194023</td>\n",
       "      <td>[7455, 45829, V1259, 2724]</td>\n",
       "      <td>[N02B, A01A, A02B, A06A, A12A, B05C, A12C, C01...</td>\n",
       "      <td>[3571, 3961, 8872]</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>21</td>\n",
       "      <td>109451</td>\n",
       "      <td>[41071, 78551, 5781, 5849, 40391, 4280, 4592, ...</td>\n",
       "      <td>[A06A, C07A, A12A, A02A, J01M, C02A, B05C, B01...</td>\n",
       "      <td>[0066, 3761, 3950, 3606, 0042, 0047, 3895, 399...</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>21</td>\n",
       "      <td>111970</td>\n",
       "      <td>[0388, 78552, 40391, 42731, 70709, 5119, 6823,...</td>\n",
       "      <td>[A06A, B05C, A12C, A07A, N02B, B01A, N06A, A01...</td>\n",
       "      <td>[3995, 8961, 0014]</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>23</td>\n",
       "      <td>124321</td>\n",
       "      <td>[2252, 3485, 78039, 4241, 4019, 2720, 2724, V4...</td>\n",
       "      <td>[C07A, N02B, A02B, H03A, N03A, A01A, N05A, C09...</td>\n",
       "      <td>[0151]</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SUBJECT_ID  HADM_ID                                          ICD9_CODE  \\\n",
       "0          17   161087  [4239, 5119, 78551, 4589, 311, 7220, 71946, 2724]   \n",
       "1          17   194023                         [7455, 45829, V1259, 2724]   \n",
       "2          21   109451  [41071, 78551, 5781, 5849, 40391, 4280, 4592, ...   \n",
       "3          21   111970  [0388, 78552, 40391, 42731, 70709, 5119, 6823,...   \n",
       "4          23   124321  [2252, 3485, 78039, 4241, 4019, 2720, 2724, V4...   \n",
       "\n",
       "                                                 NDC  \\\n",
       "0  [N02B, A01A, A02B, A06A, B05C, A12A, A12C, C01...   \n",
       "1  [N02B, A01A, A02B, A06A, A12A, B05C, A12C, C01...   \n",
       "2  [A06A, C07A, A12A, A02A, J01M, C02A, B05C, B01...   \n",
       "3  [A06A, B05C, A12C, A07A, N02B, B01A, N06A, A01...   \n",
       "4  [C07A, N02B, A02B, H03A, N03A, A01A, N05A, C09...   \n",
       "\n",
       "                                            PRO_CODE  NDC_Len  \n",
       "0                                 [3731, 8872, 3893]       14  \n",
       "1                                 [3571, 3961, 8872]       15  \n",
       "2  [0066, 3761, 3950, 3606, 0042, 0047, 3895, 399...       17  \n",
       "3                                 [3995, 8961, 0014]       17  \n",
       "4                                             [0151]       11  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# files can be downloaded from https://mimic.physionet.org/gettingstarted/dbsetup/\n",
    "med_file = 'PRESCRIPTIONS.csv'\n",
    "diag_file = 'DIAGNOSES_ICD.csv'\n",
    "procedure_file = 'PROCEDURES_ICD.csv'\n",
    "\n",
    "# drug code mapping files (already in ./data/)\n",
    "ndc2atc_file = 'ndc2atc_level4.csv' \n",
    "cid_atc = 'drug-atc.csv'\n",
    "ndc2rxnorm_file = 'ndc2rxnorm_mapping.txt'\n",
    "\n",
    "# drug-drug interactions can be down https://www.dropbox.com/s/8os4pd2zmp2jemd/drug-DDI.csv?dl=0\n",
    "ddi_file = 'drug-DDI.csv'\n",
    "\n",
    "def process_procedure():\n",
    "    pro_pd = pd.read_csv(procedure_file, dtype={'ICD9_CODE':'category'})\n",
    "    pro_pd.drop(columns=['ROW_ID'], inplace=True)\n",
    "#     pro_pd = pro_pd[pro_pd['SEQ_NUM']<5]\n",
    "#     def icd9_tree(x):\n",
    "#         if x[0]=='E':\n",
    "#             return x[:4] \n",
    "#         return x[:3]\n",
    "#     pro_pd['ICD9_CODE'] = pro_pd['ICD9_CODE'].map(icd9_tree)\n",
    "    pro_pd.drop_duplicates(inplace=True)\n",
    "    pro_pd.sort_values(by=['SUBJECT_ID', 'HADM_ID', 'SEQ_NUM'], inplace=True)\n",
    "    pro_pd.drop(columns=['SEQ_NUM'], inplace=True)\n",
    "    pro_pd.drop_duplicates(inplace=True)\n",
    "    pro_pd.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    return pro_pd\n",
    "\n",
    "\n",
    "def process_med():\n",
    "    med_pd = pd.read_csv(med_file, dtype={'NDC':'category'})\n",
    "    # filter\n",
    "    med_pd.drop(columns=['ROW_ID','DRUG_TYPE','DRUG_NAME_POE','DRUG_NAME_GENERIC',\n",
    "                     'FORMULARY_DRUG_CD','GSN','PROD_STRENGTH','DOSE_VAL_RX',\n",
    "                     'DOSE_UNIT_RX','FORM_VAL_DISP','FORM_UNIT_DISP','FORM_UNIT_DISP',\n",
    "                      'ROUTE','ENDDATE','DRUG'], axis=1, inplace=True)\n",
    "    med_pd.drop(index = med_pd[med_pd['NDC'] == '0'].index, axis=0, inplace=True)\n",
    "    med_pd.fillna(method='pad', inplace=True)\n",
    "    med_pd.dropna(inplace=True)\n",
    "    med_pd.drop_duplicates(inplace=True)\n",
    "    med_pd['ICUSTAY_ID'] = med_pd['ICUSTAY_ID'].astype('int64')\n",
    "    med_pd['STARTDATE'] = pd.to_datetime(med_pd['STARTDATE'], format='%Y-%m-%d %H:%M:%S')    \n",
    "    med_pd.sort_values(by=['SUBJECT_ID', 'HADM_ID', 'ICUSTAY_ID', 'STARTDATE'], inplace=True)\n",
    "    med_pd = med_pd.reset_index(drop=True)\n",
    "    \n",
    "    def filter_first24hour_med(med_pd):\n",
    "        med_pd_new = med_pd.drop(columns=['NDC'])\n",
    "        med_pd_new = med_pd_new.groupby(by=['SUBJECT_ID','HADM_ID','ICUSTAY_ID']).head([1]).reset_index(drop=True)\n",
    "        med_pd_new = pd.merge(med_pd_new, med_pd, on=['SUBJECT_ID','HADM_ID','ICUSTAY_ID','STARTDATE'])\n",
    "        med_pd_new = med_pd_new.drop(columns=['STARTDATE'])\n",
    "        return med_pd_new\n",
    "    med_pd = filter_first24hour_med(med_pd)\n",
    "#     med_pd = med_pd.drop(columns=['STARTDATE'])\n",
    "    \n",
    "    med_pd = med_pd.drop(columns=['ICUSTAY_ID'])\n",
    "    med_pd = med_pd.drop_duplicates()\n",
    "    med_pd = med_pd.reset_index(drop=True)\n",
    "    \n",
    "    # visit > 2\n",
    "    def process_visit_lg2(med_pd):\n",
    "        a = med_pd[['SUBJECT_ID', 'HADM_ID']].groupby(by='SUBJECT_ID')['HADM_ID'].unique().reset_index()\n",
    "        a['HADM_ID_Len'] = a['HADM_ID'].map(lambda x:len(x))\n",
    "        a = a[a['HADM_ID_Len'] > 1]\n",
    "        return a \n",
    "    med_pd_lg2 = process_visit_lg2(med_pd).reset_index(drop=True)    \n",
    "    med_pd = med_pd.merge(med_pd_lg2[['SUBJECT_ID']], on='SUBJECT_ID', how='inner')    \n",
    "    \n",
    "    return med_pd.reset_index(drop=True)\n",
    "\n",
    "def process_diag():\n",
    "    diag_pd = pd.read_csv(diag_file)\n",
    "    diag_pd.dropna(inplace=True)\n",
    "#     def icd9_tree(x):\n",
    "#         if x[0]=='E':\n",
    "#             return x[:4] \n",
    "#         return x[:3]\n",
    "#     diag_pd['ICD9_CODE'] = diag_pd['ICD9_CODE'].map(icd9_tree)\n",
    "#     diag_pd = diag_pd[diag_pd['SEQ_NUM'] < 5]\n",
    "    diag_pd.drop(columns=['SEQ_NUM','ROW_ID'],inplace=True)\n",
    "    diag_pd.drop_duplicates(inplace=True)\n",
    "    diag_pd.sort_values(by=['SUBJECT_ID','HADM_ID'], inplace=True)\n",
    "    return diag_pd.reset_index(drop=True)\n",
    "\n",
    "def ndc2atc4(med_pd):\n",
    "    with open(ndc2rxnorm_file, 'r') as f:\n",
    "        ndc2rxnorm = eval(f.read())\n",
    "    med_pd['RXCUI'] = med_pd['NDC'].map(ndc2rxnorm)\n",
    "    med_pd.dropna(inplace=True)\n",
    "\n",
    "    rxnorm2atc = pd.read_csv(ndc2atc_file)\n",
    "    rxnorm2atc = rxnorm2atc.drop(columns=['YEAR','MONTH','NDC'])\n",
    "    rxnorm2atc.drop_duplicates(subset=['RXCUI'], inplace=True)\n",
    "    med_pd.drop(index = med_pd[med_pd['RXCUI'].isin([''])].index, axis=0, inplace=True)\n",
    "    \n",
    "    med_pd['RXCUI'] = med_pd['RXCUI'].astype('int64')\n",
    "    med_pd = med_pd.reset_index(drop=True)\n",
    "    med_pd = med_pd.merge(rxnorm2atc, on=['RXCUI'])\n",
    "    med_pd.drop(columns=['NDC', 'RXCUI'], inplace=True)\n",
    "    med_pd = med_pd.rename(columns={'ATC4':'NDC'})\n",
    "    med_pd['NDC'] = med_pd['NDC'].map(lambda x: x[:4])\n",
    "    med_pd = med_pd.drop_duplicates()    \n",
    "    med_pd = med_pd.reset_index(drop=True)\n",
    "    return med_pd\n",
    "\n",
    "def filter_1000_most_pro(pro_pd):\n",
    "    pro_count = pro_pd.groupby(by=['ICD9_CODE']).size().reset_index().rename(columns={0:'count'}).sort_values(by=['count'],ascending=False).reset_index(drop=True)\n",
    "    pro_pd = pro_pd[pro_pd['ICD9_CODE'].isin(pro_count.loc[:1000, 'ICD9_CODE'])]\n",
    "    \n",
    "    return pro_pd.reset_index(drop=True)    \n",
    "\n",
    "def filter_2000_most_diag(diag_pd):\n",
    "    diag_count = diag_pd.groupby(by=['ICD9_CODE']).size().reset_index().rename(columns={0:'count'}).sort_values(by=['count'],ascending=False).reset_index(drop=True)\n",
    "    diag_pd = diag_pd[diag_pd['ICD9_CODE'].isin(diag_count.loc[:1999, 'ICD9_CODE'])]\n",
    "    \n",
    "    return diag_pd.reset_index(drop=True)\n",
    "\n",
    "def filter_300_most_med(med_pd):\n",
    "    med_count = med_pd.groupby(by=['NDC']).size().reset_index().rename(columns={0:'count'}).sort_values(by=['count'],ascending=False).reset_index(drop=True)\n",
    "    med_pd = med_pd[med_pd['NDC'].isin(med_count.loc[:299, 'NDC'])]\n",
    "    \n",
    "    return med_pd.reset_index(drop=True)\n",
    "\n",
    "def process_all():\n",
    "    # get med and diag (visit>=2)\n",
    "    med_pd = process_med()\n",
    "    med_pd = ndc2atc4(med_pd)\n",
    "#     med_pd = filter_300_most_med(med_pd)\n",
    "    \n",
    "    diag_pd = process_diag()\n",
    "    diag_pd = filter_2000_most_diag(diag_pd)\n",
    "    \n",
    "    pro_pd = process_procedure()\n",
    "#     pro_pd = filter_1000_most_pro(pro_pd)\n",
    "    \n",
    "    med_pd_key = med_pd[['SUBJECT_ID', 'HADM_ID']].drop_duplicates()\n",
    "    diag_pd_key = diag_pd[['SUBJECT_ID', 'HADM_ID']].drop_duplicates()\n",
    "    pro_pd_key = pro_pd[['SUBJECT_ID', 'HADM_ID']].drop_duplicates()\n",
    "    \n",
    "    combined_key = med_pd_key.merge(diag_pd_key, on=['SUBJECT_ID', 'HADM_ID'], how='inner')\n",
    "    combined_key = combined_key.merge(pro_pd_key, on=['SUBJECT_ID', 'HADM_ID'], how='inner')\n",
    "    \n",
    "    diag_pd = diag_pd.merge(combined_key, on=['SUBJECT_ID', 'HADM_ID'], how='inner')\n",
    "    med_pd = med_pd.merge(combined_key, on=['SUBJECT_ID', 'HADM_ID'], how='inner')\n",
    "    pro_pd = pro_pd.merge(combined_key, on=['SUBJECT_ID', 'HADM_ID'], how='inner')\n",
    "\n",
    "    # flatten and merge\n",
    "    diag_pd = diag_pd.groupby(by=['SUBJECT_ID','HADM_ID'])['ICD9_CODE'].unique().reset_index()  \n",
    "    med_pd = med_pd.groupby(by=['SUBJECT_ID', 'HADM_ID'])['NDC'].unique().reset_index()\n",
    "    pro_pd = pro_pd.groupby(by=['SUBJECT_ID','HADM_ID'])['ICD9_CODE'].unique().reset_index().rename(columns={'ICD9_CODE':'PRO_CODE'})  \n",
    "    med_pd['NDC'] = med_pd['NDC'].map(lambda x: list(x))\n",
    "    pro_pd['PRO_CODE'] = pro_pd['PRO_CODE'].map(lambda x: list(x))\n",
    "    data = diag_pd.merge(med_pd, on=['SUBJECT_ID', 'HADM_ID'], how='inner')\n",
    "    data = data.merge(pro_pd, on=['SUBJECT_ID', 'HADM_ID'], how='inner')\n",
    "#     data['ICD9_CODE_Len'] = data['ICD9_CODE'].map(lambda x: len(x))\n",
    "    data['NDC_Len'] = data['NDC'].map(lambda x: len(x))\n",
    "    return data\n",
    "\n",
    "def statistics():\n",
    "    print('#patients ', data['SUBJECT_ID'].unique().shape)\n",
    "    print('#clinical events ', len(data))\n",
    "    \n",
    "    diag = data['ICD9_CODE'].values\n",
    "    med = data['NDC'].values\n",
    "    pro = data['PRO_CODE'].values\n",
    "    \n",
    "    unique_diag = set([j for i in diag for j in list(i)])\n",
    "    unique_med = set([j for i in med for j in list(i)])\n",
    "    unique_pro = set([j for i in pro for j in list(i)])\n",
    "    \n",
    "    print('#diagnosis ', len(unique_diag))\n",
    "    print('#med ', len(unique_med))\n",
    "    print('#procedure', len(unique_pro))\n",
    "    \n",
    "    avg_diag = 0\n",
    "    avg_med = 0\n",
    "    avg_pro = 0\n",
    "    max_diag = 0\n",
    "    max_med = 0\n",
    "    max_pro = 0\n",
    "    cnt = 0\n",
    "    max_visit = 0\n",
    "    avg_visit = 0\n",
    "\n",
    "    for subject_id in data['SUBJECT_ID'].unique():\n",
    "        item_data = data[data['SUBJECT_ID'] == subject_id]\n",
    "        x = []\n",
    "        y = []\n",
    "        z = []\n",
    "        visit_cnt = 0\n",
    "        for index, row in item_data.iterrows():\n",
    "            visit_cnt += 1\n",
    "            cnt += 1\n",
    "            x.extend(list(row['ICD9_CODE']))\n",
    "            y.extend(list(row['NDC']))\n",
    "            z.extend(list(row['PRO_CODE']))\n",
    "        x = set(x)\n",
    "        y = set(y)\n",
    "        z = set(z)\n",
    "        avg_diag += len(x)\n",
    "        avg_med += len(y)\n",
    "        avg_pro += len(z)\n",
    "        avg_visit += visit_cnt\n",
    "        if len(x) > max_diag:\n",
    "            max_diag = len(x)\n",
    "        if len(y) > max_med:\n",
    "            max_med = len(y) \n",
    "        if len(z) > max_pro:\n",
    "            max_pro = len(z)\n",
    "        if visit_cnt > max_visit:\n",
    "            max_visit = visit_cnt\n",
    "        \n",
    "\n",
    "        \n",
    "    print('#avg of diagnoses ', avg_diag/ cnt)\n",
    "    print('#avg of medicines ', avg_med/ cnt)\n",
    "    print('#avg of procedures ', avg_pro/ cnt)\n",
    "    print('#avg of vists ', avg_visit/ len(data['SUBJECT_ID'].unique()))\n",
    "    \n",
    "\n",
    "    print('#max of diagnoses ', max_diag)\n",
    "    print('#max of medicines ', max_med)\n",
    "    print('#max of procedures ', max_pro)\n",
    "    print('#max of visit ', max_visit)\n",
    "    \n",
    "    \n",
    "    \n",
    "data = process_all()\n",
    "statistics()\n",
    "data.to_pickle('data_final.pkl')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Vocaboray for Medical Codes & Save Patient Record in pickle form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1958, 145, 1426)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dill\n",
    "import pandas as pd\n",
    "class Voc(object):\n",
    "    def __init__(self):\n",
    "        self.id2word = {}\n",
    "        self.word2id = {}\n",
    "\n",
    "    def add_sentence(self, sentence):\n",
    "        for word in sentence:\n",
    "            if word not in self.word2id:\n",
    "                self.id2word[len(self.word2id)] = word\n",
    "                self.word2id[word] = len(self.word2id)\n",
    "                \n",
    "def create_str_token_mapping(df):\n",
    "    diag_voc = Voc()\n",
    "    med_voc = Voc()\n",
    "    pro_voc = Voc()\n",
    "    ## only for DMNC\n",
    "#     diag_voc.add_sentence(['seperator', 'decoder_point'])\n",
    "#     med_voc.add_sentence(['seperator', 'decoder_point'])\n",
    "#     pro_voc.add_sentence(['seperator', 'decoder_point'])\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        diag_voc.add_sentence(row['ICD9_CODE'])\n",
    "        med_voc.add_sentence(row['NDC'])\n",
    "        pro_voc.add_sentence(row['PRO_CODE'])\n",
    "    \n",
    "    dill.dump(obj={'diag_voc':diag_voc, 'med_voc':med_voc ,'pro_voc':pro_voc}, file=open('voc_final.pkl','wb'))\n",
    "    return diag_voc, med_voc, pro_voc\n",
    "\n",
    "def create_patient_record(df, diag_voc, med_voc, pro_voc):\n",
    "    records = [] # (patient, code_kind:3, codes)  code_kind:diag, proc, med\n",
    "    for subject_id in df['SUBJECT_ID'].unique():\n",
    "        item_df = df[df['SUBJECT_ID'] == subject_id]\n",
    "        patient = []\n",
    "        for index, row in item_df.iterrows():\n",
    "            admission = []\n",
    "            admission.append([diag_voc.word2id[i] for i in row['ICD9_CODE']])\n",
    "            admission.append([pro_voc.word2id[i] for i in row['PRO_CODE']])\n",
    "            admission.append([med_voc.word2id[i] for i in row['NDC']])\n",
    "            patient.append(admission)\n",
    "        records.append(patient) \n",
    "    dill.dump(obj=records, file=open('records_final.pkl', 'wb'))\n",
    "    return records\n",
    "        \n",
    "    \n",
    "path='data_final.pkl'\n",
    "df = pd.read_pickle(path)\n",
    "diag_voc, med_voc, pro_voc = create_str_token_mapping(df)\n",
    "records = create_patient_record(df, diag_voc, med_voc, pro_voc)\n",
    "len(diag_voc.id2word), len(med_voc.id2word), len(pro_voc.id2word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DDI & Construct EHR Adj and DDI Adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Voc' object has no attribute 'idx2word'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-f2a164679cb2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mcid2atc_dic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mmed_voc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdill\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvoc_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'med_voc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mmed_voc_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmed_voc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midx2word\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0mmed_unique_word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmed_voc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midx2word\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmed_voc_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0matc3_atc4_dic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Voc' object has no attribute 'idx2word'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import dill\n",
    "\n",
    "# atc -> cid\n",
    "ddi_file = 'drug-DDI.csv'\n",
    "cid_atc = 'drug-atc.csv'\n",
    "voc_file = 'voc_final.pkl'\n",
    "data_path = 'records_final.pkl'\n",
    "TOPK = 40 # topk drug-drug interaction\n",
    "\n",
    "records =  dill.load(open(data_path, 'rb'))\n",
    "cid2atc_dic = defaultdict(set)\n",
    "med_voc = dill.load(open(voc_file, 'rb'))['med_voc']\n",
    "med_voc_size = len(med_voc.id2word)\n",
    "med_unique_word = [med_voc.id2word[i] for i in range(med_voc_size)]\n",
    "atc3_atc4_dic = defaultdict(set)\n",
    "for item in med_unique_word:\n",
    "    atc3_atc4_dic[item[:4]].add(item)\n",
    "    \n",
    "\n",
    "with open(cid_atc, 'r') as f:\n",
    "    for line in f:\n",
    "        line_ls = line[:-1].split(',')\n",
    "        cid = line_ls[0]\n",
    "        atcs = line_ls[1:]\n",
    "        for atc in atcs:\n",
    "            if len(atc3_atc4_dic[atc[:4]]) != 0:\n",
    "                cid2atc_dic[cid].add(atc[:4])\n",
    "            \n",
    "# ddi load\n",
    "ddi_df = pd.read_csv(ddi_file)\n",
    "# fliter sever side effect \n",
    "ddi_most_pd = ddi_df.groupby(by=['Polypharmacy Side Effect', 'Side Effect Name']).size().reset_index().rename(columns={0:'count'}).sort_values(by=['count'],ascending=False).reset_index(drop=True)\n",
    "ddi_most_pd = ddi_most_pd.iloc[-TOPK:,:]\n",
    "# ddi_most_pd = pd.DataFrame(columns=['Side Effect Name'], data=['as','asd','as'])\n",
    "fliter_ddi_df = ddi_df.merge(ddi_most_pd[['Side Effect Name']], how='inner', on=['Side Effect Name'])\n",
    "ddi_df = fliter_ddi_df[['STITCH 1','STITCH 2']].drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "# weighted ehr adj \n",
    "ehr_adj = np.zeros((med_voc_size, med_voc_size))\n",
    "for patient in records:\n",
    "    for adm in patient:\n",
    "        med_set = adm[2]\n",
    "        for i, med_i in enumerate(med_set):\n",
    "            for j, med_j in enumerate(med_set):\n",
    "                if j<=i:\n",
    "                    continue\n",
    "                ehr_adj[med_i, med_j] = 1\n",
    "                ehr_adj[med_j, med_i] = 1\n",
    "dill.dump(ehr_adj, open('ehr_adj_final.pkl', 'wb'))  \n",
    "\n",
    "\n",
    "\n",
    "# ddi adj\n",
    "ddi_adj = np.zeros((med_voc_size,med_voc_size))\n",
    "for index, row in ddi_df.iterrows():\n",
    "    # ddi\n",
    "    cid1 = row['STITCH 1']\n",
    "    cid2 = row['STITCH 2']\n",
    "    \n",
    "    # cid -> atc_level3\n",
    "    for atc_i in cid2atc_dic[cid1]:\n",
    "        for atc_j in cid2atc_dic[cid2]:\n",
    "            \n",
    "            # atc_level3 -> atc_level4\n",
    "            for i in atc3_atc4_dic[atc_i]:\n",
    "                for j in atc3_atc4_dic[atc_j]:\n",
    "                    if med_voc.word2idx[i] != med_voc.word2idx[j]:\n",
    "                        ddi_adj[med_voc.word2idx[i], med_voc.word2idx[j]] = 1\n",
    "                        ddi_adj[med_voc.word2idx[j], med_voc.word2idx[i]] = 1\n",
    "dill.dump(ddi_adj, open('ddi_A_final.pkl', 'wb')) \n",
    "                        \n",
    "print('complete!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
